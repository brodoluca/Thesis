@article{ayaz_internet--things_2019,
	title = {Internet-of-{Things} ({IoT})-{Based} {Smart} {Agriculture}: {Toward} {Making} the {Fields} {Talk}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Internet-of-{Things} ({IoT})-{Based} {Smart} {Agriculture}},
	url = {https://ieeexplore.ieee.org/document/8784034/},
	doi = {10.1109/ACCESS.2019.2932609},
	urldate = {2021-08-20},
	journal = {IEEE Access},
	author = {Ayaz, Muhammad and Ammad-Uddin, Mohammad and Sharif, Zubair and Mansour, Ali and Aggoune, El-Hadi M.},
	year = {2019},
	pages = {129551--129583},
	file = {Full Text:/Users/brodie/Zotero/storage/UGFYYHFS/Ayaz et al. - 2019 - Internet-of-Things (IoT)-Based Smart Agriculture .pdf:application/pdf},
}
@article{bhadra_weed_2020,
	title = {Weed management in sugar beet: {A} review},
	volume = {5},
	issn = {2518-2021},
	shorttitle = {Weed management in sugar beet},
	url = {https://www.ejmanager.com/fulltextpdf.php?mno=83758},
	doi = {10.5455/faa.83758},
	number = {0},
	urldate = {2021-08-18},
	journal = {Fundamental and Applied Agriculture},
	author = {Bhadra, Tamalika and Paul, Swapan},
	year = {2020},
	pages = {1},
	file = {Full Text:/Users/brodie/Zotero/storage/EC52NVIS/Bhadra e Paul - 2020 - Weed management in sugar beet A review.pdf:application/pdf},
}
@article{schweizer_weed_1989,
	title = {"{Weed} control in sugarbeets ({Beta} vulgaris) in {North} {America}"},
	author = {Schweizer, EE and Dexter, A.G},
	year = {1989},
}
@article{may_economic_2003,
	title = {Economic consequences for {UK} farmers of growing {GM} herbicide tolerant sugar beet},
	volume = {142},
	issn = {0003-4746, 1744-7348},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1744-7348.2003.tb00227.x},
	doi = {10.1111/j.1744-7348.2003.tb00227.x},
	language = {en},
	number = {1},
	urldate = {2021-08-20},
	journal = {Annals of Applied Biology},
	author = {May, M J},
	month = feb,
	year = {2003},
	pages = {41--48},
}
@article{singh_machine_2016,
	title = {Machine {Learning} for {High}-{Throughput} {Stress} {Phenotyping} in {Plants}},
	volume = {21},
	issn = {1360-1385},
	url = {https://www.sciencedirect.com/science/article/pii/S1360138515002630},
	doi = {https://doi.org/10.1016/j.tplants.2015.10.015},
	abstract = {Advances in automated and high-throughput imaging technologies have resulted in a deluge of high-resolution images and sensor data of plants. However, extracting patterns and features from this large corpus of data requires the use of machine learning (ML) tools to enable data assimilation and feature identification for stress phenotyping. Four stages of the decision cycle in plant stress phenotyping and plant breeding activities where different ML approaches can be deployed are (i) identification, (ii) classification, (iii) quantification, and (iv) prediction (ICQP). We provide here a comprehensive overview and user-friendly taxonomy of ML tools to enable the plant community to correctly and easily apply the appropriate ML tools and best-practice guidelines for various biotic and abiotic stress traits.},
	number = {2},
	journal = {Trends in Plant Science},
	author = {Singh, Arti and Ganapathysubramanian, Baskar and Singh, Asheesh Kumar and Sarkar, Soumik},
	year = {2016},
	keywords = {abiotic stress, biotic stress, high-throughput phenotyping, Imaging, machine learning, plant breeding},
	pages = {110--124},
}
@misc{masclef_sugar_1891,
	title = {Sugar {Beet}},
	url = {https://commons.wikimedia.org/w/index.php?curid=5767279},
	author = {Masclef, Amédée},
	year = {1891},
}
@inproceedings{lottes_effective_2016,
	address = {Stockholm, Sweden},
	title = {An effective classification system for separating sugar beets and weeds for precision farming applications},
	isbn = {978-1-4673-8026-3},
	url = {http://ieeexplore.ieee.org/document/7487720/},
	doi = {10.1109/ICRA.2016.7487720},
	urldate = {2021-08-16},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Lottes, P. and Hoeferlin, M. and Sander, S. and Muter, M. and Schulze, P. and Stachniss, Lammers C.},
	month = may,
	year = {2016},
	pages = {5157--5163},
	annote = {Weed identification using mobile robot
USes RF, howver since the robot is very close to the plants, they are able to identify the vegetation using a special type of camera.},
}

@article{islam_review_2021,
	title = {A {Review} of {Applications} and {Communication} {Technologies} for {Internet} of {Things} ({IoT}) and {Unmanned} {Aerial} {Vehicle} ({UAV}) {Based} {Sustainable} {Smart} {Farming}},
	volume = {13},
	issn = {2071-1050},
	url = {https://www.mdpi.com/2071-1050/13/4/1821},
	doi = {10.3390/su13041821},
	abstract = {To reach the goal of sustainable agriculture, smart farming is taking advantage of the Unmanned Aerial Vehicles (UAVs) and Internet of Things (IoT) paradigm. These smart farms are designed to be run by interconnected devices and vehicles. Some enormous potentials can be achieved by the integration of different IoT technologies to achieve automated operations with minimum supervision. This paper outlines some major applications of IoT and UAV in smart farming, explores the communication technologies, network functionalities and connectivity requirements for Smart farming. The connectivity limitations of smart agriculture and it’s solutions are analysed with two case studies. In case study-1, we propose and evaluate meshed Long Range Wide Area Network (LoRaWAN) gateways to address connectivity limitations of Smart Farming. While in case study-2, we explore satellite communication systems to provide connectivity to smart farms in remote areas of Australia. Finally, we conclude the paper by identifying future research challenges on this topic and outlining directions to address those challenges.},
	language = {en},
	number = {4},
	urldate = {2021-08-16},
	journal = {Sustainability},
	author = {Islam, Nahina and Rashid, Md Mamunur and Pasandideh, Faezeh and Ray, Biplob and Moore, Steven and Kadel, Rajan},
	month = feb,
	year = {2021},
	pages = {1821},
	annote = {Applications of technologies in smart farming.
It's a general overview},
	file = {Full Text:/Users/brodie/Zotero/storage/8HD6EJ8M/Islam et al. - 2021 - A Review of Applications and Communication Technol.pdf:application/pdf},
}

@article{glaroudis_survey_2020,
	title = {Survey, comparison and research challenges of {IoT} application protocols for smart farming},
	volume = {168},
	issn = {13891286},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128619306942},
	doi = {10.1016/j.comnet.2019.107037},
	language = {en},
	urldate = {2021-08-20},
	journal = {Computer Networks},
	author = {Glaroudis, Dimitrios and Iossifides, Athanasios and Chatzimisios, Periklis},
	month = feb,
	year = {2020},
	pages = {107037},
}





@article{Fleming1986HowNT,
  title={How not to lie with statistics: the correct way to summarize benchmark results},
  author={Philip J. Fleming and John J. Wallace},
  journal={Commun. ACM},
  year={1986},
  volume={29},
  pages={218-221}
}


@article{DBLP:journals/corr/abs-1811-01412,
  author    = {Martin Becker and
               Samarjit Chakraborty},
  title     = {Measuring Software Performance on Linux},
  journal   = {CoRR},
  volume    = {abs/1811.01412},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.01412},
  eprinttype = {arXiv},
  eprint    = {1811.01412},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-01412.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{Real-Time-Systems,
  author={Wägemann, Peter and Distler, Tobias and Eichler, Christian and Schröder-Preikschat, Wolfgang},
  booktitle={2017 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)}, 
  title={Benchmark Generation for Timing Analysis}, 
  year={2017},
  volume={},
  number={},
  pages={319-330},
  doi={10.1109/RTAS.2017.6}}



@inproceedings{how_to_bench,
author = {von Kistowski, Jóakim and Arnold, Jeremy and Huppler, Karl and Lange, Klaus-Dieter and Henning, John and Cao, Paul},
year = {2015},
month = {02},
pages = {},
title = {How to Build a Benchmark},
journal = {ICPE 2015 - Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
doi = {10.1145/2668930.2688819}
}


@inproceedings{Stewart2001MeasuringET,
  title={Measuring Execution Time and Real-Time Performance},
  author={David B. Stewart},
  year={2001}
}



@book{linux_commands,
author = {Kerrisk, Michael},
title = {The Linux Programming Interface: A Linux and UNIX System Programming Handbook},
year = {2010},
isbn = {1593272200},
publisher = {No Starch Press},
address = {USA},
edition = {1st},
abstract = {The Linux Programming Interface is the definitive guide to the Linux and UNIX programming
interfacethe interface employed by nearly every application that runs on a Linux or
UNIX system. In this authoritative work, Linux programming expert Michael Kerrisk
provides detailed descriptions of the system calls and library functions that you
need in order to master the craft of system programming, and accompanies his explanations
with clear, complete example programs. You'll find descriptions of over 500 system
calls and library functions, and more than 200 example programs, 88 tables, and 115
diagrams. You'll learn how to: Read and write files efficiently Use signals, clocks,
and timers Create processes and execute programs Write secure programs Write multithreaded
programs using POSIX threads Build and use shared libraries Perform interprocess communication
using pipes, message queues, shared memory, and semaphores Write network applications
with the sockets API While The Linux Programming Interface covers a wealth of Linux-specific
features, including epoll, inotify, and the /proc file system, its emphasis on UNIX
standards (POSIX.1-2001/SUSv3 and POSIX.1-2008/SUSv4) makes it equally valuable to
programmers working on other UNIX platforms. The Linux Programming Interface is the
most comprehensive single-volume work on the Linux and UNIX programming interface,
and a book that's destined to become a new classic. Praise for The Linux Programming
Interface "If I had to choose a single book to sit next to my machine when writing
software for Linux, this would be it." Martin Landers, Software Engineer, Google "This
book, with its detailed descriptions and examples, contains everything you need to
understand the details and nuances of the low-level programming APIs in Linux . .
. no matter what the level of reader, there will be something to be learnt from this
book." Mel Gorman, Author of Understanding the Linux Virtual Memory Manager "Michael
Kerrisk has not only written a great book about Linux programming and how it relates
to various standards, but has also taken care that bugs he noticed got fixed and the
man pages were (greatly) improved. In all three ways, he has made Linux programming
easier. The in-depth treatment of topics in The Linux Programming Interface . . .
makes it a must-have reference for both new and experienced Linux programmers." Andreas
Jaeger, Program Manager, openSUSE, Novell "Michael's inexhaustible determination to
get his information right, and to express it clearly and concisely, has resulted in
a strong reference source for programmers. While this work is targeted at Linux programmers,
it will be of value to any programmer working in the UNIX/POSIX ecosystem." David
Butenhof, Author of Programming with POSIX Threads and Contributor to the POSIX and
UNIX Standards ". . . a very thoroughyet easy to readexplanation of UNIX system and
network programming, with an emphasis on Linux systems. It's certainly a book I'd
recommend to anybody wanting to get into UNIX programming (in general) or to experienced
UNIX programmers wanting to know 'what's new' in the popular GNU/Linux system." Fernando
Gont, Network Security Researcher, IETF Participant, and RFC Author ". . . encyclopedic
in the breadth and depth of its coverage, and textbook-like in its wealth of worked
examples and exercises. Each topic is clearly and comprehensively covered, from theory
to hands-on working code. Professionals, students, educators, this is the Linux/UNIX
reference that you have been waiting for." Anthony Robins, Associate Professor of
Computer Science, The University of Otago "I've been very impressed by the precision,
the quality and the level of detail Michael Kerrisk put in his book. He is a great
expert of Linux system calls and lets us share his knowledge and understanding of
the Linux APIs." Christophe Blaess, Author of Programmation systeme en C sous Linux
". . . an essential resource for the serious or professional Linux and UNIX systems
programmer. Michael Kerrisk covers the use of all the key APIs across both the Linux
and UNIX system interfaces with clear descriptions and tutorial examples and stresses
the importance and benefits of following standards such as the Single UNIX Specification
and POSIX 1003.1." Andrew Josey, Director, Standards, The Open Group, and Chair of
the POSIX 1003.1 Working Group "What could be better than an encyclopedic reference
to the Linux system, from the standpoint of the system programmer, written by none
other than the maintainer of the man pages himself? The Linux Programming Interface
is comprehensive and detailed. I firmly expect it to become an indispensable addition
to my programming bookshelf." Bill Gallmeister, Author of POSIX.4 Programmer's Guide:
Programming for the Real World ". . . the most complete and up-to-date book about
Linux and UNIX system programming. If you're new to Linux system programming, if you're
a UNIX veteran focused on portability while interested in learning the Linux way,
or if you're simply looking for an excellent reference about the Linux programming
interface, then Michael Kerrisk's book is definitely the companion you want on your
bookshelf." Loic Domaigne, Chief Software Architect (Embedded), Corpuls.com}
}


@article{Beyer2017ReliableBR,
  title={Reliable benchmarking: requirements and solutions},
  author={Dirk Beyer and Stefan L{\"o}we and Philipp Wendler},
  journal={International Journal on Software Tools for Technology Transfer},
  year={2017},
  volume={21},
  pages={1-29}
}


@online{LinuxManualWeb,
  author = {Michael Kerrisk},
  title = {Linux Manual Page},
  year = 2021,
  url = {https://man7.org/linux/man-pages/man7/cgroups.7.html},
  urldate = {2021-11-04}
}



%%%%%%%%%%%
%Benchmarking Neural Networks

@ARTICLE{Confidence_Interval,
  author={Chryssolouris, G. and Lee, M. and Ramsey, A.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Confidence interval prediction for neural network models}, 
  year={1996},
  volume={7},
  number={1},
  pages={229-232},
  doi={10.1109/72.478409}}



@misc{gawlikowski2021survey,
      title={A Survey of Uncertainty in Deep Neural Networks}, 
      author={Jakob Gawlikowski and Cedrique Rovile Njieutcheu Tassi and Mohsin Ali and Jongseok Lee and Matthias Humt and Jianxiang Feng and Anna Kruspe and Rudolph Triebel and Peter Jung and Ribana Roscher and Muhammad Shahzad and Wen Yang and Richard Bamler and Xiao Xiang Zhu},
      year={2021},
      eprint={2107.03342},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{zhang2021ai,
      title={The AI Index 2021 Annual Report}, 
      author={Daniel Zhang and Saurabh Mishra and Erik Brynjolfsson and John Etchemendy and Deep Ganguli and Barbara Grosz and Terah Lyons and James Manyika and Juan Carlos Niebles and Michael Sellitto and Yoav Shoham and Jack Clark and Raymond Perrault},
      year={2021},
      eprint={2103.06312},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@article{Number_of_DL_papers,
author = {Song, Kyoung and Kim, Myeongchan and Do, Synho},
year = {2019},
month = {03},
pages = {202},
title = {The Latest Trends in the Use of Deep Learning in Radiology Illustrated Through the Stages of Deep Learning Algorithm Development},
volume = {80},
journal = {Journal of the Korean Society of Radiology},
doi = {10.3348/jksr.2019.80.2.202}
}

@misc{ovadia2019trust,
      title={Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift}, 
      author={Yaniv Ovadia and Emily Fertig and Jie Ren and Zachary Nado and D Sculley and Sebastian Nowozin and Joshua V. Dillon and Balaji Lakshminarayanan and Jasper Snoek},
      year={2019},
      eprint={1906.02530},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@misc{ruder2017overview,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{nguyen2015deep,
      title={Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}, 
      author={Anh Nguyen and Jason Yosinski and Jeff Clune},
      year={2015},
      eprint={1412.1897},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}



@INPROCEEDINGS{Separation_uncer,
  author={Huseljic, Denis and Sick, Bernhard and Herde, Marek and Kottke, Daniel},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Separation of Aleatoric and Epistemic Uncertainty in Deterministic Deep Neural Networks}, 
  year={2021},
  volume={},
  number={},
  pages={9172-9179},
  doi={10.1109/ICPR48806.2021.9412616}}
@article{uncertainity_classi,
author = {Hüllermeier, Eyke and Waegeman, Willem},
year = {2021},
month = {03},
pages = {},
title = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
volume = {110},
journal = {Machine Learning},
doi = {10.1007/s10994-021-05946-3}
}

@article{DBLP:journals/corr/KendallG17,
  author    = {Alex Kendall and
               Yarin Gal},
  title     = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer
               Vision?},
  journal   = {CoRR},
  volume    = {abs/1703.04977},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.04977},
  eprinttype = {arXiv},
  eprint    = {1703.04977},
  timestamp = {Mon, 13 Aug 2018 16:48:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KendallG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{KIUREGHIAN2009105,
title = {Aleatory or epistemic? Does it matter?},
journal = {Structural Safety},
volume = {31},
number = {2},
pages = {105-112},
year = {2009},
note = {Risk Acceptance and Risk Communication},
issn = {0167-4730},
doi = {https://doi.org/10.1016/j.strusafe.2008.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167473008000556},
author = {Armen Der Kiureghian and Ove Ditlevsen},
keywords = {Aleatory, Epistemic, Ergodicity, Parameter uncertainty, Predictive models, Probability distribution choice, Statistical dependence, Systems, Time-variant reliability, Uncertainty},
abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems.}
}

@article{bianco2018dnnsbench,
 author = {Bianco, Simone and Cadene, Remi and Celona, Luigi and Napoletano, Paolo},
 year = {2018},
 title = {Benchmark Analysis of Representative Deep Neural Network Architectures},
 journal = {IEEE Access},
 volume = {6},
 pages = {64270-64277},
 doi = {10.1109/ACCESS.2018.2877890},
 ISSN = {2169-3536},
}
@misc{luo2020comparison,
      title={Comparison and Benchmarking of AI Models and Frameworks on Mobile Devices}, 
      author={Chunjie Luo and Xiwen He and Jianfeng Zhan and Lei Wang and Wanling Gao and Jiahui Dai},
      year={2020},
      eprint={2005.05085},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{reddi2020mlperf,
      title={MLPerf Inference Benchmark}, 
      author={Vijay Janapa Reddi and Christine Cheng and David Kanter and Peter Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody Coleman and Sam Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and J. Scott Gardner and Itay Hubara and Sachin Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and Colin Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem Wu and Lingjie Xu and Koichi Yamada and Bing Yu and George Yuan and Aaron Zhong and Peizhao Zhang and Yuchen Zhou},
      year={2020},
      eprint={1911.02549},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{ignatov2019ai,
      title={AI Benchmark: All About Deep Learning on Smartphones in 2019}, 
      author={Andrey Ignatov and Radu Timofte and Andrei Kulik and Seungsoo Yang and Ke Wang and Felix Baum and Max Wu and Lirong Xu and Luc Van Gool},
      year={2019},
      eprint={1910.06663},
      archivePrefix={arXiv},
      primaryClass={cs.PF}
}
@inproceedings{Coleman2017DAWNBenchA,
  title={DAWNBench : An End-to-End Deep Learning Benchmark and Competition},
  author={Cody A. Coleman and Deepak Narayanan and Daniel Kang and Tian Zhao and Jian Zhang and Luigi Nardi and Peter Bailis and Kunle Olukotun and Christopher R{\'e} and Matei A. Zaharia},
  year={2017}
}

@misc{unterthiner2021predicting,
      title={Predicting Neural Network Accuracy from Weights}, 
      author={Thomas Unterthiner and Daniel Keysers and Sylvain Gelly and Olivier Bousquet and Ilya Tolstikhin},
      year={2021},
      eprint={2002.11448},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{OH20041311,
title = {GPU implementation of neural networks},
journal = {Pattern Recognition},
volume = {37},
number = {6},
pages = {1311-1314},
year = {2004},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2004.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0031320304000524},
author = {Kyoung-Su Oh and Keechul Jung},
keywords = {Graphics processing unit(GPU), Neural network(NN), Multi-layer perceptron, Text detection},
abstract = {Graphics processing unit (GPU) is used for a faster artificial neural network. It is used to implement the matrix multiplication of a neural network to enhance the time performance of a text detection system. Preliminary results produced a 20-fold performance enhancement using an ATI RADEON 9700 PRO board. The parallelism of a GPU is fully utilized by accumulating a lot of input feature vectors and weight vectors, then converting the many inner-product operations into one matrix operation. Further research areas include benchmarking the performance with various hardware and GPU-aware learning algorithms.}
}

@article{DBLP:journals/corr/CanzianiPC16,
  author    = {Alfredo Canziani and
               Adam Paszke and
               Eugenio Culurciello},
  title     = {An Analysis of Deep Neural Network Models for Practical Applications},
  journal   = {CoRR},
  volume    = {abs/1605.07678},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.07678},
  eprinttype = {arXiv},
  eprint    = {1605.07678},
  timestamp = {Mon, 13 Aug 2018 16:46:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/CanzianiPC16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10.1007/978-3-642-04274-4_39,
author="Guzhva, Alexander
and Dolenko, Sergey
and Persiantsev, Igor",
editor="Alippi, Cesare
and Polycarpou, Marios
and Panayiotou, Christos
and Ellinas, Georgios",
title="Multifold Acceleration of Neural Network Computations Using GPU",
booktitle="Artificial Neural Networks -- ICANN 2009",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="373--380",
abstract="With emergence of graphics processing units (GPU) of the latest generation, it became possible to undertake neural network based computations using GPU on serially produced video display adapters. In this study, NVIDIA CUDA technology has been used to implement standard back-propagation algorithm for training multiple perceptrons simultaneously on GPU. For the problem considered, GPU-based implementation (on NVIDIA GTX 260 GPU) has lead to a 50x speed increase compared to a highly optimized CPU-based computer program, and more than 150x compared to a commercially available CPU-based software (NeuroShell 2) (AMD Athlon 64 Dual core 6000+ processor).",
isbn="978-3-642-04274-4"
}

@inproceedings{10.1145/3089801.3089804, author = {Cao, Qingqing and Balasubramanian, Niranjan and Balasubramanian, Aruna}, title = {MobiRNN: Efficient Recurrent Neural Network Execution on Mobile GPU}, year = {2017}, isbn = {9781450349628}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3089801.3089804}, doi = {10.1145/3089801.3089804}, abstract = {In this paper, we explore optimizations to run Recurrent Neural Network (RNN) models locally on mobile devices. RNN models are widely used for Natural Language Processing, Machine translation, and other tasks. However, existing mobile applications that use RNN models do so on the cloud. To address privacy and efficiency concerns, we show how RNN models can be run locally on mobile devices. Existing work on porting deep learning models to mobile devices focus on Convolution Neural Networks (CNNs) and cannot be applied directly to RNN models. In response, we present MobiRNN, a mobile-specific optimization framework that implements GPU offloading specifically for mobile GPUs. Evaluations using an RNN model for activity recognition shows that MobiRNN does significantly decrease the latency of running RNN models on phones.}, booktitle = {Proceedings of the 1st International Workshop on Deep Learning for Mobile Systems and Applications}, pages = {1–6}, numpages = {6}, keywords = {recurrent neural network, renderscript, mobile GPU, performance optimizations}, location = {Niagara Falls, New York, USA}, series = {EMDL '17} }
 
@misc{paine2013gpu,
      title={GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training}, 
      author={Thomas Paine and Hailin Jin and Jianchao Yang and Zhe Lin and Thomas Huang},
      year={2013},
      eprint={1312.6186},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@INPROCEEDINGS{8090194,
  author={Cengil, Emine and Cinar, Ahmet and Gueler, Zafer},
  booktitle={2017 International Artificial Intelligence and Data Processing Symposium (IDAP)}, 
  title={A GPU-based convolutional neural network approach for image classification}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/IDAP.2017.8090194}}

@online{Correct_inference_measure,
  author = { Amnon Geifman },
  title = {The Correct Way to Measure Inference Time of Deep Neural Networks},
  year = 2021,
  url = {https://deci.ai/resources/blog/measure-inference-time-deep-neural-networks/},
  urldate = {2020-04-04}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@online{intel_bench_suite,
  author = { Gennady Fedorov Shaojuan Zhu and Abhinav Singh },
  title = {Intel® oneAPI Math Kernel Library (oneMKL) Benchmarks Suite},
  year = 2021,
  url = {https://www.intel.com/content/www/us/en/developer/articles/technical/onemkl-benchmarks-suite.html},
  urldate = {2021-11-22}
}
@INPROCEEDINGS{118273,
  author={Perugini and Engeler},
  booktitle={International 1989 Joint Conference on Neural Networks}, 
  title={Neural network learning time: effects of network and training set size}, 
  year={1989},
  volume={},
  number={},
  pages={395-401 vol.2},
  doi={10.1109/IJCNN.1989.118273}}

@INPROCEEDINGS{8573476,
  author={Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
  booktitle={2018 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Benchmarking and Analyzing Deep Neural Network Training}, 
  year={2018},
  volume={},
  number={},
  pages={88-100},
  doi={10.1109/IISWC.2018.8573476}}
@misc{han2016eie,
      title={EIE: Efficient Inference Engine on Compressed Deep Neural Network}, 
      author={Song Han and Xingyu Liu and Huizi Mao and Jing Pu and Ardavan Pedram and Mark A. Horowitz and William J. Dally},
      year={2016},
      eprint={1602.01528},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@book{machine_learning,
  title = {Machine Learning},
  author = {Tom M Mitchell},
  year = {1997},
  publisher = {New York  McGraw-Hill},
}
@article{murphy2016overview,
  title={An overview of convolutional neural network architectures for deep learning},
  author={Murphy, John},
  journal={Microway Inc},
  year={2016}
}
@misc{rhu2016vdnn,
      title={vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design}, 
      author={Minsoo Rhu and Natalia Gimelshein and Jason Clemons and Arslan Zulfiqar and Stephen W. Keckler},
      year={2016},
      eprint={1602.08124},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}
@misc{bojarski2016end,
      title={End to End Learning for Self-Driving Cars}, 
      author={Mariusz Bojarski and Davide Del Testa and Daniel Dworakowski and Bernhard Firner and Beat Flepp and Prasoon Goyal and Lawrence D. Jackel and Mathew Monfort and Urs Muller and Jiakai Zhang and Xin Zhang and Jake Zhao and Karol Zieba},
      year={2016},
      eprint={1604.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{huval2015empirical,
      title={An Empirical Evaluation of Deep Learning on Highway Driving}, 
      author={Brody Huval and Tao Wang and Sameep Tandon and Jeff Kiske and Will Song and Joel Pazhayampallil and Mykhaylo Andriluka and Pranav Rajpurkar and Toki Migimatsu and Royce Cheng-Yue and Fernando Mujica and Adam Coates and Andrew Y. Ng},
      year={2015},
      eprint={1504.01716},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
@proceedings{10.1145/2959100,
title = {RecSys '16: Proceedings of the 10th ACM Conference on Recommender Systems},
year = {2016},
isbn = {9781450340359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 10th ACM Conference on Recommender Systems (RecSys 2016), held in Boston, MA, USA, from September 15th through 19th. Since it was first held ten years ago, RecSys has grown to become the leading conference for the presentation and discussion of recommender systems research, bringing together the world's leading recommender systems researchers and e-commerce companies.The program for RecSys 2016 reflects the growth of the Recommender Systems community. For the second time in the history of RecSys we will offer two parallel tracks during the three days of the main conference with a record breaking 112 contributions including 51 technical papers, 9 Past, Present, and Future papers, 15 industry papers, four tutorials, three keynotes, and 30 demos and posters. We again offer an extensive pre-conference program with nine workshops, the RecSys Challenge and a doctoral symposium.The technical program for RecSys 2016 drew upon a record 294 total submissions. To celebrate the tenth year of the conference, the program features a new track reflecting on past, present, and future research in the field of recommender systems. Papers in this track consider a broad perspective on how the field has evolved and the challenges and directions that lie ahead. The review process for all tracks was highly selective. In the main program, 29 long papers were accepted out of 159 submissions (18.2% acceptance rate), 22 out of 110 short papers (20% acceptance rate), and 9 out of 25 Past, Present and Future paper submissions (36% acceptance rate). Prominent topics covered by these papers include human factors, social aspects, context awareness, cold start, novelty and diversity, and core algorithmic research (matrix factorization, deep learning, probabilistic approaches, etc.).Building on the tradition established by previous years, RecSys 2016 features a strong focus on significant real-world challenges facing industrial practitioners and practical solutions to those challenges. The three industry sessions feature a rich set of talks from Mendeley, Meetup, Bloomberg, Foursquare, Spotify, Netflix, Pandora, Stitch Fix, Expedia, Nara Logics, GraphSQL, Retail Rocket, Quora, Google and Pinterest. A wide range of domains are represented in these sessions including publishing, news, Q &amp; A, events, music, movies, television, fashion, apps and games.},
location = {Boston, Massachusetts, USA}
}

@misc{amodei2015deep,
      title={Deep Speech 2: End-to-End Speech Recognition in English and Mandarin}, 
      author={Dario Amodei and Rishita Anubhai and Eric Battenberg and Carl Case and Jared Casper and Bryan Catanzaro and Jingdong Chen and Mike Chrzanowski and Adam Coates and Greg Diamos and Erich Elsen and Jesse Engel and Linxi Fan and Christopher Fougner and Tony Han and Awni Hannun and Billy Jun and Patrick LeGresley and Libby Lin and Sharan Narang and Andrew Ng and Sherjil Ozair and Ryan Prenger and Jonathan Raiman and Sanjeev Satheesh and David Seetapun and Shubho Sengupta and Yi Wang and Zhiqian Wang and Chong Wang and Bo Xiao and Dani Yogatama and Jun Zhan and Zhenyao Zhu},
      year={2015},
      eprint={1512.02595},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Afaq2020SignificanceOE,
  title={Significance Of Epochs On Training A Neural Network},
  author={Saahil Afaq and Smitha Rao},
  journal={International Journal of Scientific \& Technology Research},
  year={2020},
  volume={9},
  pages={485-488}
}

@article{Unterthiner2020PredictingNN,
  title={Predicting Neural Network Accuracy from Weights},
  author={Thomas Unterthiner and Daniel Keysers and Sylvain Gelly and Olivier Bousquet and Ilya O. Tolstikhin},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.11448}
}

@misc{hooker2019benchmark,
      title={A Benchmark for Interpretability Methods in Deep Neural Networks}, 
      author={Sara Hooker and Dumitru Erhan and Pieter-Jan Kindermans and Been Kim},
      year={2019},
      eprint={1806.10758},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}